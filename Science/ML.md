http://www.machinelearning.ru/wiki/index.php?title=%D0%97%D0%B0%D0%B3%D0%BB%D0%B0%D0%B2%D0%BD%D0%B0%D1%8F_%D1%81%D1%82%D1%80%D0%B0%D0%BD%D0%B8%D1%86%D0%B0

X - множество объектов (информационных описаний)
Y - множество ответов (оценок, предсказаний или прогнозов)
$y: X \to Y$ - неизвестная зависимость (target function)

Дано
$\{x_1, x_2, \dots, x_l\} \subset X$ - обучающая выборка (training sample)
$y_i=y(x_i), i = 1, \dots, l$ - известные ответы обучающей выборки

Найти:
$a: X \to Y$ - алгоритм, решающую функцию (decision function), приближающую у на всем множестве Х. Аппроксимация (проведение) функции по заданным точкам. От классических задач аппроксимации отличается сложной природой объектов х.

Машинное обучение отвечает на вопросы:
- как задаются объекты и какими могут быть ответы
- что значит что функция а приближает функцию у
- как строить функцию а

$f_j: X \to D_j, j = 1, \dots, n$ - признаки объектов (результаты измерений характеристик объектов) (features)

# Типы признаков
- $D_j=\{0,1\}$ - бинарный признак $f_j$
- $|D_j|< \infty$ - номинальный признак $f_j$ (когда число ответов конечно, пр. оценки студента по разным курсам, шкала от 1 до 5)
- $|D_j|< \infty$, $D_j$ упорядочено - порядковый признак $f_j$
- $D_j=R$ - количественный признак $f_j$

Вектор $(f_1(x), ..., f_n(x))$ - признаковое описание объекта х
Матрица "объекты-признаки" (feature data)
$$F=||f_j(x_i)||_{l\cdot n}=
\begin{bmatrix}
f_1(x_1) & \dots & f_n(x_1)\\ 
\dots & \dots & \dots \\
f_1(x_l) & \dots & f_n(x_l)
\end{bmatrix}$$
тут в строках объекты, в столбцах признаки.

# Как задаются ответы
- Задачи обучения с учителем (обучение по прецедентам, восстановление зависимостей по эмпирическим данным, предсказательное моделирование, аппроксимация функций по заданным точкам)
	- Задачи классификации (classification):
	 ![[classification.png|center|150]]
		- $Y=\{-1,+1\}$ - классификация на 2 класса
			-  Человек пришел за кредитом (объект) и решение банка о выдаче кредита (ответ)
			- Болен или нет (ответ) пациент (объект)
		- $Y=\{1, \dots, M\}$ - на М непересекающихся классов
			- Распознавание символов документа (объект) при сканировании, в каждой позиции стоит только один символ из М классов (количество кодов в Юникоде) 
		- $Y=\{0, 1\}^M$ - на М классов, которые могут пересекаться
			- Когда пациент (объект) болен несколькими болезнями (ответ)
	- Задачи восстановления регрессии (regression):
 ![[regression.png| center | 150]]
		- $Y=R$ или $Y=R^m$
	- Задачи ранжирования (ranking, learning to rank):
		- $Y$ - конечное упорядоченное множество
			- поиск в google с приоритетом
- Задачи обучения без учителя (unsupervised learning):
	-  Ответов нет, требуется что-то делать с самими объектами

# Предсказательная модель
Модель (predictive model) - параметрическое семейство функций
$A=\{g(x,\theta)|\theta \in \Theta\}$, где $g: X \cdot \Theta \to Y$ - фиксированная функция
$\Theta$ - множество допустимых значений параметра $\theta$
Функция g известна, а $\theta$ - нет, то есть используем известную модель g с неизвестным параметром $\theta$ (применяем стратегию разделяй и властвуй)

Пример: Линейная модель с вектором параметров $\theta=(\theta_1, \theta_2, \dots, \theta_n\} \in R^n$:
$g(x,\theta)=\displaystyle \sum_{j = 1}^n \theta_jf_j(x)$ - для регрессии и ранжирования, $Y=R$. Нужно обучить модель, то есть решить оптимизационную задачу чтобы определить веса $\theta_j$ наилучшим образом подходящие для аппроксимации модели по обучающей выборке

$g(x,\theta)=sign \displaystyle \sum_{j = 1}^n \theta_jf_j(x)$ - для классификации, $Y=\{-1,+1\}$

# Функция потерь
$L(a, x)$ - функция потерь (loss function) величина ошибки алгоритма $a\in A$ на объекте $x\in X$

Функция потерь для задач классификации
$L(a, x) = [a(x) \ne y(x)]$ - индикатор ошибки (квадратные скобки означают "истина или ложь")

Функция потерь для задач регрессии
$L(a, x) = |a(x) - y(x)|$ - абсолютное значение ошибки
$L(a, x) = (a(x) - y(x))^2$ - квадратичная ошибка

Эмпирический риск - функционал качества алгоритма а на $X^l$:
$Q(a,X^l)={1 \over l} \displaystyle \sum_{i = 1}^n L(a, x_i)$

Метод минимизации эмпирического риска (empirical risk minimization, ERM)
$\mu(X^l)=arg \displaystyle \min_{a \in A}Q(a,X^l)$

Пример:
Задача регрессии, $Y=R$
n числовых признаков $f_j: X \to R, j = 1, ..., n$
линейная модель регрессии: $g(x_i,\theta)=\displaystyle \sum_{j = 1}^n \theta_jf_j(x), \theta \in R^n$
квадратичная функция потерь: $L(a, x) = (a(x) - y(x))^2$
метод наименьших квадратов - частный случай ERM:
$\mu(X^l)=arg \displaystyle \min_{\theta}\displaystyle \sum_{i = 1}^l(g(x_i,\theta)-y_i)^2$











